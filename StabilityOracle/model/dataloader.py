# =====================================================================================
# StabilityOracle - Dataloader (`dataloader.py`)
#
# Research Context:
# This script is the foundational data engineering pipeline for the Stability Oracle
# framework. It is responsible for implementing the "Masked Microenvironment Graph
# Generation" process (see Figure 1.a of the manuscript). This process transforms raw
# protein atomic data from JSONL files into the structured tensor formats required by
# the MutComputeXGT model defined in `blocks.py` and `model.py`.
# =====================================================================================

import json
import torch
import pandas as pd
import numpy as np


# =====================================================================================
# VOCABULARY AND MAPPINGS
# These dictionaries define the discrete vocabulary for atomic and amino acid types,
# allowing them to be converted into integer indices for use in the model's embedding layers.
# This standardization is a prerequisite for any deep learning model processing this data.
# =====================================================================================

ELEMENT_MAP = lambda x: {
    "H": 0,
    "C": 1,
    "N": 2,
    "O": 3,
    "F": 4,
    "S": 5,
    "P": 6,
    # Research Note: Halogens are grouped into a single category (index 7), a common
    # simplification in structural bioinformatics models where specific halogen
    # distinctions are less critical than their general presence.
    "F": 7,
    "Cl": 7,
    "CL": 7,
    "Br": 7,
    "BR": 7,
    "I": 7,
}.get(x, 8) # Index 8 serves as the "unknown" or "other" category for any non-standard elements.

AA_INDEX = {
   "ALA": 0,
   "ARG": 1,
   "ASN": 2,
   "ASP": 3,
   "CYS": 4,
   "GLU": 5,
   "GLN": 6,
   "GLY": 7,
   "HIS": 8,
   "ILE": 9,
   "LEU": 10,
   "LYS": 11,
   "MET": 12,
   "PHE": 13,
   "PRO": 14,
   "SER": 15,
   "THR": 16,
   "TRP": 17,
   "TYR": 18,
   "VAL": 19,
}
# Utility dictionary to map 3-letter amino acid codes to their standard 1-letter representations.
AA_3to1 = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',
    'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N',
    'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W',
    'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}

AMINO_ACIDS = list(AA_INDEX.keys())

# =====================================================================================
# DATA LOADING FUNCTIONS
# Defines two distinct pipelines for loading graph data. The choice of which function
# to use is determined in `pipeline.py` by inspecting the input JSONL file's structure.
# =====================================================================================

def load_embedded_graph(jsonl: dict, device: torch.device=None) -> dict:
    """
    Research Note:
    This function defines the "Embedded Graph" format. It is a utility pipeline for loading
    data that has already been featurized. It bypasses the raw PDB parsing and feature
    calculation steps, instead expecting a pre-computed feature vector for each node in
    the 'input' key of the JSONL. This is likely used for faster inference or for experiments
    where features are generated by an external tool or a different model.
    """
    feats = []
    coords = []
    mask = []
    cas = []
    label = []
    from_aas = []
    to_aas = []
    mut_infos = []
    pdb_codes = []


    for data_idx in range(len(jsonl)):
        example = json.loads(jsonl[data_idx])

        mut_infos.append(example["mut_info"])
        pdb_codes.append(example["pdb_id"])
        # The key difference: node features are loaded directly from the 'input' key.
        feats.append(example["input"])
        coords.append(example["coords"])
        mask.append(example["mask"])
        cas.append(example["ca"])
        # The model is trained to predict -ΔΔG, as is conventional in the field.
        label.append(-example["ddg"])
        from_aas.append(example["from"])
        to_aas.append(example["to"])

    return {
        "feats": feats,
        "coords": coords,
        "mask": mask,
        "cas": cas,
        "label": label,
        "from_aas": from_aas,
        "to_aas": to_aas,
        "mut_infos": mut_infos,
        "pdb_codes": pdb_codes,
    }


def load_raw_graph(jsonl: dict, device: torch.device=None) -> dict:
    """
    Research Note:
    This function implements the primary "Raw Graph" format and is the core data pipeline
    described in the manuscript. It takes raw atomic data from a PDB structure (via the
    'atomic_collection' key) and executes the full "Masked Microenvironment Graph Generation"
    process, featurizing all required tensors from scratch.
    """
    # A fixed size for padding. All tensors will be padded to this length to enable
    # batching for GPU processing.
    max_atoms = 512

    # Initialize lists to hold tensors for each protein structure in the input.
    PPs = []          # Physical Properties (Charge, SASA)
    ATs = []          # Atom Types
    COORDs = []       # Atom Coordinates
    masks = []        # Padding Masks
    CAs = []          # Alpha Carbon Coordinates of the mutation site
    labels = []       # Experimental ΔΔG values
    mutation_AAs = [] # Pairs of [wild_type_idx, mutant_idx]
    mut_infos = []    # Mutation strings (e.g., "A123C_A")
    pdb_codes = []    # PDB identifiers


    for data_idx in range(len(jsonl)):
        data = json.loads(jsonl[data_idx])

        # `ss` (snapshot) contains metadata about the PDB structure. The `.get()` method
        # is used for safe dictionary access, preventing errors if a key is missing.
        ss = data.get("snapshot", {})
        filename = ss.get("filename", "")
        model_id = ss.get("model_id", 1)

        # The 'atomic_collection' contains the raw atomic data (element, coordinates, charge, etc.).
        atoms = pd.DataFrame.from_dict(data["atomic_collection"])
        ca_idx = data.get("target_alpha_carbon", None)
        # `masked_aa_atom_idx` holds the indices of the atoms belonging to the wild-type residue
        # at the mutation site. These will be removed.
        masked_aa_atom_idx = set(data["target"])


        wt_AA = ss.get("wt_aa", None)
        if wt_AA is None: wt_AA = ss.get("label", "")

        # --- Masked Microenvironment Implementation ---
        # Research Note: As detailed in the manuscript, the model learns from the structural "socket"
        # by removing the atoms of the wild-type residue. This is a core concept used in two distinct training phases:
        #
        # 1. Pre-training (Self-Supervised): The model is given only this masked environment and
        #    must predict the identity of the removed wild-type residue. This forces it to learn
        #    a rich representation of structural and chemical compatibility.
        # 2. Fine-tuning (Supervised, for ΔΔG): The model uses its pre-trained understanding
        #    of the "socket" to compare the wild-type and mutant residues, predicting the
        #    resulting change in stability from the environment's interaction with a potential new residue.
        atom_index = {idx for idx in range(len(atoms))}
        # The set of target residue atoms is subtracted from the set of all atom indices.
        atom_index = list(atom_index - masked_aa_atom_idx)

        # The alpha-carbon of the mutation site serves as a fixed anchor point for the environment.
        ca = (
            torch.as_tensor(
                [
                    atoms["x"][ca_idx],
                    atoms["y"][ca_idx],
                    atoms["z"][ca_idx],
                ],
                dtype=torch.float32,
            )
            .unsqueeze(0)
            .repeat(20, 1) # Repeated for the 20-way mutation batching.
        )

        pos = atoms["res_seq_num"][ca_idx]
        chain_id = atoms["chain_id"][ca_idx]

        # The `atoms` DataFrame is now filtered to contain only the atoms of the environment.
        atoms = atoms.iloc[atom_index].reset_index(drop=True)
        # Truncate the environment if it exceeds the maximum allowed number of atoms.
        if atoms.shape[0] > max_atoms:
            atoms = atoms[: max_atoms]

        # --- Tensor Featurization from Raw Data ---
        # 1. Atomic Coordinates
        coords = torch.as_tensor(
            atoms[["x", "y", "z"]].to_numpy(), dtype=torch.float32
        )

        # This is a zero-padding operation. It ensures that every coordinate tensor in the
        # batch has the exact same size (`max_atoms`), which is required for GPU processing.
        coords = (
            torch.cat(
                # Creates a tensor of zeros for the missing atoms to reach `max_atoms`.
                (coords, torch.zeros([max_atoms - coords.shape[0], 3])),
                dim=0,
            )
            .float()
            .unsqueeze(0)
            .repeat(20, 1, 1) # Repeated for the 20-way batching.
        )

        # 2. Atom Types (Discrete Features)
        atom_types = torch.as_tensor(
            list(map(ELEMENT_MAP, atoms.element)), dtype=torch.long
        )

        # This is the corresponding zero-padding operation for the atom types tensor.
        atom_types = (
            torch.cat(
                (
                    atom_types,
                    # Creates a 1D tensor of zeros with the correct integer type.
                    torch.zeros([max_atoms - atom_types.shape[0]]).long(),
                ),
                dim=0,
            )
            .unsqueeze(0)
            .repeat(20, 1) # Repeated for the 20-way batching.
        )

        # 3. Biophysical Properties (Continuous Features)
        add_props = ['_atom_site.fw2_charge', '_atom_site.FreeSASA_value']
        # Data cleaning: handle missing values ('?' or '.') from PDB files by replacing with 0.
        for prop in add_props:
            atoms.loc[atoms[prop].isin(['?', '.']), prop] = 0.0
            atoms[prop] = atoms[prop].astype(float)

        pp = torch.as_tensor(atoms[add_props].to_numpy(),dtype=torch.float32,)
        # This is the zero-padding operation for the physical properties tensor.
        pp = (
            # Concatenates a zero tensor for the missing atoms. The shape is [num_missing, 2]
            # because there are two physical properties (charge and SASA).
            torch.cat((pp, torch.zeros([max_atoms - pp.shape[0], 2])), dim=0)
            .float()
            .unsqueeze(0)
            .repeat(20, 1, 1) # Repeated for the 20-way batching.
        )

        # 4. Padding Mask ("Red-Herring")
        # A tensor of 1s and 0s to inform the attention mechanism which nodes are real atoms vs. padding.
        # Research Note on the `mask` tensor:
        #
        # At first glance, this tensor appears to be an incorrect padding mask, as it assigns
        # a value of '1' to all atom positions, including those that are padded with zeros.
        # This is intentional but potentially confusing.
        #
        # This `mask` tensor is NOT the final attention mask used by the Transformer layers.
        # Instead, the true, effective padding and neighborhood masking is handled dynamically
        # inside the `Backbone.forward` method in `blocks.py`. There, a new mask is created
        # based on the pairwise 3D distances between atoms (`relative_dis`). Padded atoms,
        # having coordinates at the origin, will be "too far" from real atoms and will thus
        # be implicitly masked out by the distance-based attention bias.
        #
        # Therefore, this tensor of all ones should be considered a placeholder. Its original
        # purpose may have been for an experimental feature (e.g., explicitly masking out
        # certain real atoms), but in the final model, its role is superseded by the more
        # sophisticated dynamic masking in the `Backbone`.
        mask = torch.ones([max_atoms]).float().unsqueeze(0).repeat(20, 1)

        wt_AA = ss['label']

        # --- 20-Way Mutation Batching for Efficiency ---
        # To avoid running the model 20 times for each site, a single batch of 20 is created.
        # Each item in the batch represents the same environment but is paired with a different
        # target amino acid. This allows for prediction of all 20 ΔΔG values
        # in one forward pass. This is a key contributor to the framework's computational
        # efficiency (see Supplementary Table 1 for performance benchmarks).
        
        # The following line of code creates a tensor of shape [20, 2]. Each row contains the
        # integer index for the wild-type amino acid and the integer index for one
        # of the 20 possible amino acids.
        from_to_pairs = torch.as_tensor(
        # For every possible target amino acid, look up its integer index in AA_INDEX
            [ [ AA_INDEX[wt_AA], AA_INDEX[to_AA] ] for to_AA in AMINO_ACIDS],
            dtype=torch.long,
        )
        pdb_code = filename[:4]
        pos = ss.get('res_seq_num', 0)
        chain_id = ss.get('chain_id', 'A')
        # Generate standard mutation notation (e.g., A123C_A) for each of the 20 possibilities.
        mut_info = [AA_3to1[wt_AA] + str(pos) + AA_3to1[to_AA] + '_' + chain_id for to_AA in AMINO_ACIDS]
        # Experimental ΔΔG value, if available.
        label = ss.get('ddg', np.nan)

        # Append the processed tensors for this site to the overall batch lists.
        ATs.append(atom_types)
        COORDs.append(coords)
        CAs.append(ca)
        PPs.append(pp)
        masks.append(mask)
        mutation_AAs.append( from_to_pairs )
        # Append metadata 20 times to match the repeated tensors.
        pdb_codes += [ pdb_code ] * 20
        mut_infos += mut_info
        labels += [label] * 20


    # Collate the lists of tensors into single, large batch tensors ready for the model.
    return {
        'atom_types': torch.cat(ATs, dim=0),
        'coords': torch.cat(COORDs, dim=0),
        'cas': torch.cat(CAs, dim=0),
        'pp': torch.cat(PPs, dim=0),
        'mask': torch.cat(masks, dim=0),
        'input_aa': torch.cat(mutation_AAs, dim=0),
        'pdb_codes': pdb_codes,
        'mut_infos': mut_infos,
        'label': labels
    }
